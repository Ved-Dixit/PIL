{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /opt/anaconda3/lib/python3.12/site-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from evaluate) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /opt/anaconda3/lib/python3.12/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/anaconda3/lib/python3.12/site-packages (from evaluate) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/anaconda3/lib/python3.12/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from evaluate) (0.29.3)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from evaluate) (24.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.10.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.12/site-packages (3.4.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: reportlab in /opt/anaconda3/lib/python3.12/site-packages (4.3.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from reportlab) (10.4.0)\n",
      "Requirement already satisfied: chardet in /opt/anaconda3/lib/python3.12/site-packages (from reportlab) (4.0.0)\n",
      "Requirement already satisfied: datetime in /opt/anaconda3/lib/python3.12/site-packages (5.5)\n",
      "Requirement already satisfied: zope.interface in /opt/anaconda3/lib/python3.12/site-packages (from datetime) (5.4.0)\n",
      "Requirement already satisfied: pytz in /opt/anaconda3/lib/python3.12/site-packages (from datetime) (2024.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from zope.interface->datetime) (75.1.0)\n",
      "Requirement already satisfied: flask in /opt/anaconda3/lib/python3.12/site-packages (3.0.3)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from flask) (3.0.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from flask) (3.1.4)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from flask) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from flask) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /opt/anaconda3/lib/python3.12/site-packages (from flask) (1.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from Jinja2>=3.1.2->flask) (2.1.3)\n",
      "Requirement already satisfied: flask-cors in /opt/anaconda3/lib/python3.12/site-packages (5.0.1)\n",
      "Requirement already satisfied: flask>=0.9 in /opt/anaconda3/lib/python3.12/site-packages (from flask-cors) (3.0.3)\n",
      "Requirement already satisfied: Werkzeug>=0.7 in /opt/anaconda3/lib/python3.12/site-packages (from flask-cors) (3.0.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from flask>=0.9->flask-cors) (3.1.4)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from flask>=0.9->flask-cors) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from flask>=0.9->flask-cors) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /opt/anaconda3/lib/python3.12/site-packages (from flask>=0.9->flask-cors) (1.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from Werkzeug>=0.7->flask-cors) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate\n",
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install reportlab\n",
    "!pip install datetime\n",
    "!pip install flask\n",
    "!pip install flask-cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from evaluate import load as load_metric\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from datetime import datetime\n",
    "from reportlab.lib.pagesizes import A4\n",
    "from reportlab.pdfgen import canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Loading cleaned dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9047942695344c31ad22d48e15bacc46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# ✅ Step 1: Load & Clean Dataset\n",
    "# ---------------------------\n",
    "\n",
    "print(\"📘 Loading cleaned dataset...\")\n",
    "\n",
    "# Load the dataset in JSONL format\n",
    "dataset = load_dataset(\"json\", data_files=\"data/legal_pil_dataset_fixed.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Loading GPT-2 model...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# 🔧 Step 2: Load GPT-2 Model & Tokenizer\n",
    "# ---------------------------\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "print(\"🚀 Loading GPT-2 model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# ✅ Fix padding issue\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✂️ Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512c920566654ff085ab27bbc3da4362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1477 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 🔥 Step 3: Tokenize & Setup Training Data\n",
    "# ---------------------------\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize and set input labels for GPT-2 to compute loss.\"\"\"\n",
    "    tokenized_output = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=1024,\n",
    "    )\n",
    "    # ✅ Set labels for loss calculation\n",
    "    tokenized_output[\"labels\"] = tokenized_output[\"input_ids\"].copy()\n",
    "    return tokenized_output\n",
    "\n",
    "print(\"✂️ Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Starting GPT-2 fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/2954 [05:34<91:29:29, 111.61s/it]\n",
      " 17%|█▋        | 500/2954 [1:05:24<5:18:16,  7.78s/it]\n",
      " 17%|█▋        | 500/2954 [1:05:24<5:18:16,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3172, 'grad_norm': 0.7413356304168701, 'learning_rate': 4.1536899119837506e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                           \n",
      "                                                      \n",
      " 17%|█▋        | 500/2954 [1:46:03<5:18:16,  7.78s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.27264729142189026, 'eval_runtime': 2439.4708, 'eval_samples_per_second': 0.605, 'eval_steps_per_second': 0.076, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 1000/2954 [2:51:46<4:09:51,  7.67s/it]  \n",
      " 34%|███▍      | 1000/2954 [2:51:46<4:09:51,  7.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.289, 'grad_norm': 0.5477713942527771, 'learning_rate': 3.3073798239675016e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                           \n",
      "                                                       \n",
      " 34%|███▍      | 1000/2954 [3:32:10<4:09:51,  7.67s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2496897429227829, 'eval_runtime': 2424.5825, 'eval_samples_per_second': 0.609, 'eval_steps_per_second': 0.076, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 1500/2954 [4:36:42<3:10:00,  7.84s/it]   \n",
      " 51%|█████     | 1500/2954 [4:36:42<3:10:00,  7.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2932, 'grad_norm': 0.6375637054443359, 'learning_rate': 2.4610697359512526e-05, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                           \n",
      "                                                       \n",
      " 51%|█████     | 1500/2954 [5:16:42<3:10:00,  7.84s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2321518212556839, 'eval_runtime': 2399.9982, 'eval_samples_per_second': 0.615, 'eval_steps_per_second': 0.077, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 2000/2954 [6:21:31<2:04:04,  7.80s/it]   \n",
      " 68%|██████▊   | 2000/2954 [6:21:31<2:04:04,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2305, 'grad_norm': 0.7528636455535889, 'learning_rate': 1.6147596479350036e-05, 'epoch': 1.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                           \n",
      "                                                       \n",
      " 68%|██████▊   | 2000/2954 [7:01:04<2:04:04,  7.80s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22218036651611328, 'eval_runtime': 2373.3754, 'eval_samples_per_second': 0.622, 'eval_steps_per_second': 0.078, 'epoch': 1.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 2500/2954 [8:04:36<57:58,  7.66s/it]     \n",
      " 85%|████████▍ | 2500/2954 [8:04:36<57:58,  7.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2428, 'grad_norm': 2.480236768722534, 'learning_rate': 7.684495599187543e-06, 'epoch': 1.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                           \n",
      "                                                     \n",
      " 85%|████████▍ | 2500/2954 [8:45:20<57:58,  7.66s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21552439033985138, 'eval_runtime': 2444.5586, 'eval_samples_per_second': 0.604, 'eval_steps_per_second': 0.076, 'epoch': 1.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2954/2954 [9:51:18<00:00,  9.42s/it]    \n",
      "100%|██████████| 2954/2954 [9:51:20<00:00, 12.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 35480.1174, 'train_samples_per_second': 0.083, 'train_steps_per_second': 0.083, 'train_loss': 0.2722556695957481, 'epoch': 2.0}\n",
      "✅ Fine-tuning complete — model saved!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# ---------------------------\n",
    "# 🔥 Step 5: Train GPT-2\n",
    "# ---------------------------\n",
    "\n",
    "print(\"🔥 Starting GPT-2 fine-tuning...\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_pil_trained\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=2,  # Reduce if needed\n",
    "    per_device_train_batch_size=1,  # Reduce from 2 to 1\n",
    "    save_total_limit=4,  # Reduce saved checkpoints\n",
    "    save_steps=1000,  # Reduce frequency of saving\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,\n",
    "    fp16=True  # Enable mixed precision (only for GPUs)\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"train\"],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ✅ Save the fine-tuned model\n",
    "model.save_pretrained(\"./gpt2_pil_trained\")\n",
    "tokenizer.save_pretrained(\"./gpt2_pil_trained\")\n",
    "\n",
    "print(\"✅ Fine-tuning complete — model saved!\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "gpt2_pil_generator = pipeline(\"text-generation\", model=\"./gpt2_pil_trained\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_full_pil(subject, petitioner, respondent):\n",
    "    \"\"\"Generate the entire PIL document from scratch using GPT-2.\"\"\"\n",
    "    prompt = build_smart_prompt(subject)\n",
    "\n",
    "    result = gpt2_pil_generator(\n",
    "        prompt,\n",
    "        max_length=1024,\n",
    "        temperature=0.4,\n",
    "        num_return_sequences=1,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    return result[0][\"generated_text\"].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def export_pil(pil_text, filename=\"Generated_PIL_Document\"):\n",
    "    \"\"\"Export the PIL to text and PDF files.\"\"\"\n",
    "    # Save as TXT\n",
    "    with open(f\"{filename}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(pil_text)\n",
    "    print(f\"✅ PIL saved as {filename}.txt\")\n",
    "\n",
    "    # Save as PDF\n",
    "    pdf_file = f\"{filename}.pdf\"\n",
    "    c = canvas.Canvas(pdf_file, pagesize=A4)\n",
    "    c.setFont(\"Helvetica\", 12)\n",
    "    for i, line in enumerate(pil_text.split(\"\\n\")):\n",
    "        c.drawString(40, 800 - (i * 20), line)\n",
    "    c.save()\n",
    "    print(f\"✅ PIL saved as {pdf_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Loading GPT-2 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [16/Mar/2025 17:49:13] \"OPTIONS /run_pil_generator HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 Welcome to the PIL Generator!\n",
      "⚡ Generating the complete PIL document...\n",
      "⚡ Generating Legal Grounds...\n",
      "⚠️ Legal Grounds generation failed — retrying with stronger prompt...\n",
      "⚡ Generating Prayer...\n",
      "⚠️ Prayer generation failed — retrying with stronger prompt...\n",
      "⚡ Generating Court Procedure...\n",
      "⚠️ Court Procedure generation failed — retrying with stronger prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [16/Mar/2025 17:49:14] \"POST /run_pil_generator HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Mar/2025 17:52:45] \"OPTIONS /run_pil_generator HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 Welcome to the PIL Generator!\n",
      "⚡ Generating the complete PIL document...\n",
      "⚡ Generating Legal Grounds...\n",
      "⚠️ Legal Grounds generation failed — retrying with stronger prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [16/Mar/2025 17:52:46] \"POST /run_pil_generator HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ Generating Prayer...\n",
      "⚠️ Prayer generation failed — retrying with stronger prompt...\n",
      "⚡ Generating Court Procedure...\n",
      "⚠️ Court Procedure generation failed — retrying with stronger prompt...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from transformers import pipeline\n",
    "from reportlab.lib.pagesizes import A4\n",
    "from reportlab.pdfgen import canvas\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "# 🚀 Load GPT-2 model\n",
    "print(\"🚀 Loading GPT-2 model...\")\n",
    "gpt2_pil_generator = pipeline(\"text-generation\", model=\"./gpt2_pil_trained\")\n",
    "\n",
    "\n",
    "# 🎯 Improved GPT-2 generation with retries and stronger prompts\n",
    "def generate_gpt_section(prompt, section_name=\"\"):\n",
    "    \"\"\"Generate sections using GPT-2 with retries and better prompt control.\"\"\"\n",
    "    try:\n",
    "        # First attempt at generation\n",
    "        result = gpt2_pil_generator(\n",
    "            prompt,\n",
    "            max_length=512,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "        generated_text = result[0][\"generated_text\"].replace(prompt, \"\").strip()\n",
    "\n",
    "        # Retry with a stronger prompt if generation fails or gives junk\n",
    "        if not generated_text or len(generated_text) < 20:\n",
    "            print(f\"⚠️ {section_name} generation failed — retrying with stronger prompt...\")\n",
    "            refined_prompt = (\n",
    "                f\"{prompt}\\n\\nEnsure the output is legally accurate, clear, and enforceable.\"\n",
    "            )\n",
    "            refined_result = gpt2_pil_generator(\n",
    "                refined_prompt,\n",
    "                max_length=512,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.2,\n",
    "                num_return_sequences=1,\n",
    "                do_sample=True,\n",
    "                truncation=True,\n",
    "            )\n",
    "            generated_text = refined_result[0][\"generated_text\"].replace(refined_prompt, \"\").strip()\n",
    "\n",
    "        # Final fallback if both attempts fail\n",
    "        if not generated_text or len(generated_text) < 20:\n",
    "            return f\"⚠️ Unable to generate {section_name}. Please consult a legal expert.\"\n",
    "\n",
    "        return generated_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating {section_name}: {e}\")\n",
    "        return f\"⚠️ Error generating {section_name}. Please check the input.\"\n",
    "\n",
    "\n",
    "# 🛠️ Assemble the PIL Document with enhanced structure\n",
    "def generate_pil(petitioner, respondent, subject, summary):\n",
    "    \"\"\"Create the full PIL document with powerful Legal Grounds, Prayer, and Court Procedure generation.\"\"\"\n",
    "\n",
    "    print(\"⚡ Generating Legal Grounds...\")\n",
    "    legal_grounds_prompt = (\n",
    "        f\"Draft strong, persuasive legal grounds for a Public Interest Litigation (PIL) on '{subject}', \"\n",
    "        f\"citing specific constitutional articles, environmental laws (e.g., Environment Protection Act 1986), \"\n",
    "        f\"legal principles (e.g., Precautionary Principle, Polluter Pays Principle), and landmark cases (e.g., M.C. Mehta v. Union of India 1987).\"\n",
    "        f\"Ensure the argument is solid, legally backed, and supports the petitioner's cause.\"\n",
    "    )\n",
    "    legal_grounds = generate_gpt_section(legal_grounds_prompt, \"Legal Grounds\")\n",
    "\n",
    "    print(\"⚡ Generating Prayer...\")\n",
    "    prayer_prompt = (\n",
    "        f\"Draft a clear, practical, and enforceable prayer (request) for a PIL on '{subject}', \"\n",
    "        f\"including specific directives the court can issue (e.g., impose fines, enforce environmental audits, \"\n",
    "        f\"order cleanup operations, penalize non-compliant industries, ensure public data transparency).\"\n",
    "    )\n",
    "    prayer = generate_gpt_section(prayer_prompt, \"Prayer\")\n",
    "\n",
    "    print(\"⚡ Generating Court Procedure...\")\n",
    "    court_procedure_prompt = (\n",
    "        f\"Draft a legally accurate and step-by-step court procedure for filing a Public Interest Litigation (PIL) \"\n",
    "        f\"in the Supreme Court of India on '{subject}', ensuring it includes jurisdiction, notice, evidence submission, and requests for an expedited hearing.\"\n",
    "    )\n",
    "    court_procedure = generate_gpt_section(court_procedure_prompt, \"Court Procedure\")\n",
    "\n",
    "    # ✅ Fallback version of Court Procedure in case GPT fails\n",
    "    if \"⚠️\" in court_procedure:\n",
    "        court_procedure = (\n",
    "            \"1. File the PIL under Article 32 of the Constitution for Supreme Court jurisdiction.\\n\"\n",
    "            \"2. Serve notice to the Respondent and the Attorney General of India.\\n\"\n",
    "            \"3. Attach supporting evidence such as scientific data, expert affidavits, and media reports.\\n\"\n",
    "            \"4. Request expedited hearing citing urgent public interest.\"\n",
    "        )\n",
    "\n",
    "    # ✅ Assemble the final PIL document\n",
    "    pil_text = f\"\"\"\n",
    "IN THE HON'BLE SUPREME COURT OF INDIA\n",
    "\n",
    "PUBLIC INTEREST LITIGATION (PIL)\n",
    "\n",
    "Petitioner: {petitioner}\n",
    "Respondent: {respondent}\n",
    "\n",
    "Subject: {subject}\n",
    "\n",
    "Respected Lordships,\n",
    "\n",
    "{summary}\n",
    "\n",
    "Legal Grounds:\n",
    "{legal_grounds}\n",
    "\n",
    "Prayer:\n",
    "{prayer}\n",
    "\n",
    "Court Procedure:\n",
    "{court_procedure}\n",
    "\n",
    "Date: {datetime.now().strftime(\"%A, %d %B %Y\")}\n",
    "\n",
    "Yours sincerely,\n",
    "{petitioner}\n",
    "    \"\"\"\n",
    "\n",
    "    return pil_text\n",
    "\n",
    "\n",
    "# 📄 Export PIL to TXT and PDF\n",
    "def export_pil(pil_text, filename=\"PIL_Document\"):\n",
    "    \"\"\"Export the PIL to text and PDF files.\"\"\"\n",
    "    # Save as TXT\n",
    "    with open(f\"{filename}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(pil_text)\n",
    "    print(f\"✅ PIL saved as {filename}.txt\")\n",
    "\n",
    "    # Save as PDF\n",
    "    pdf_file = f\"{filename}.pdf\"\n",
    "    c = canvas.Canvas(pdf_file, pagesize=A4)\n",
    "    c.setFont(\"Helvetica\", 12)\n",
    "    for i, line in enumerate(pil_text.split(\"\\n\")):\n",
    "        c.drawString(40, 800 - (i * 20), line)\n",
    "    c.save()\n",
    "    print(f\"✅ PIL saved as {pdf_file}\")\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)  # Enable CORS for frontend communication\n",
    "\n",
    "@app.route('/run_pil_generator', methods=['POST'])\n",
    "    # 🎯 Interactive CLI for PIL Generation\n",
    "def run_pil_generator():\n",
    "    data= request.json\n",
    "    subject = data.get('subject')\n",
    "    petitioner = data.get('petitioner')\n",
    "    respondent = data.get('respondent')\n",
    "    summary = data.get('summary')\n",
    "    \"\"\"Run the PIL Generator interactively.\"\"\"\n",
    "    print(\"🎉 Welcome to the PIL Generator!\")\n",
    "\n",
    "\n",
    "    # Generate the PIL document\n",
    "    print(\"⚡ Generating the complete PIL document...\")\n",
    "    generated_pil = generate_pil(petitioner, respondent, subject, summary)\n",
    "    return jsonify({'pil-text':generated_pil})\n",
    "# 🔥 Run the Generator\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lsof: unknown protocol name (9002) in: -i 9002\n",
      "lsof 4.91\n",
      " latest revision: ftp://lsof.itap.purdue.edu/pub/tools/unix/lsof/\n",
      " latest FAQ: ftp://lsof.itap.purdue.edu/pub/tools/unix/lsof/FAQ\n",
      " latest man page: ftp://lsof.itap.purdue.edu/pub/tools/unix/lsof/lsof_man\n",
      " usage: [-?abhlnNoOPRtUvVX] [+|-c c] [+|-d s] [+D D] [+|-f[cgG]]\n",
      " [-F [f]] [-g [s]] [-i [i]] [+|-L [l]] [+|-M] [-o [o]] [-p s]\n",
      " [+|-r [t]] [-s [p:s]] [-S [t]] [-T [t]] [-u s] [+|-w] [-x [fl]] [--] [names]\n",
      "Use the ``-h'' option to get more help information.\n"
     ]
    }
   ],
   "source": [
    "!lsof -i 9002\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 167\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# 🔥 Run the Generator\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 167\u001b[0m     app\u001b[38;5;241m.\u001b[39mrun(debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/flask/app.py:625\u001b[0m, in \u001b[0;36mFlask.run\u001b[0;34m(self, host, port, debug, load_dotenv, **options)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwerkzeug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run_simple\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 625\u001b[0m     run_simple(t\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;28mstr\u001b[39m, host), port, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;66;03m# reset the first request information if the development server\u001b[39;00m\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# reset normally.  This makes it possible to restart the server\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# without reloader and that stuff from an interactive shell.\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_got_first_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/werkzeug/serving.py:1106\u001b[0m, in \u001b[0;36mrun_simple\u001b[0;34m(hostname, port, application, use_reloader, use_debugger, use_evalex, extra_files, exclude_patterns, reloader_interval, reloader_type, threaded, processes, request_handler, static_files, passthrough_errors, ssl_context)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_reloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run_with_reloader\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1106\u001b[0m     run_with_reloader(\n\u001b[1;32m   1107\u001b[0m         srv\u001b[38;5;241m.\u001b[39mserve_forever,\n\u001b[1;32m   1108\u001b[0m         extra_files\u001b[38;5;241m=\u001b[39mextra_files,\n\u001b[1;32m   1109\u001b[0m         exclude_patterns\u001b[38;5;241m=\u001b[39mexclude_patterns,\n\u001b[1;32m   1110\u001b[0m         interval\u001b[38;5;241m=\u001b[39mreloader_interval,\n\u001b[1;32m   1111\u001b[0m         reloader_type\u001b[38;5;241m=\u001b[39mreloader_type,\n\u001b[1;32m   1112\u001b[0m     )\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1114\u001b[0m     srv\u001b[38;5;241m.\u001b[39mserver_close()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/werkzeug/_reloader.py:458\u001b[0m, in \u001b[0;36mrun_with_reloader\u001b[0;34m(main_func, extra_files, exclude_patterns, interval, reloader_type)\u001b[0m\n\u001b[1;32m    456\u001b[0m             reloader\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 458\u001b[0m         sys\u001b[38;5;241m.\u001b[39mexit(reloader\u001b[38;5;241m.\u001b[39mrestart_with_reloader())\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mSystemExit\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: transformers[torch]\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Loading GPT-2 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5001\n",
      "Press CTRL+C to quit\n",
      " * Restarting with watchdog (fsevents)\n",
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1074, in launch_instance\n",
      "    app.initialize(argv)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 118, in inner\n",
      "    return method(app, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 654, in initialize\n",
      "    self.init_sockets()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 331, in init_sockets\n",
      "    self.shell_port = self._bind_socket(self.shell_socket, self.shell_port)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 253, in _bind_socket\n",
      "    return self._try_bind_socket(s, port)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 229, in _try_bind_socket\n",
      "    s.bind(\"tcp://%s:%i\" % (self.ip, port))\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/zmq/sugar/socket.py\", line 302, in bind\n",
      "    super().bind(addr)\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 564, in zmq.backend.cython.socket.Socket.bind\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 28, in zmq.backend.cython.checkrc._check_rc\n",
      "zmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002')\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from transformers import pipeline\n",
    "from reportlab.lib.pagesizes import A4\n",
    "from reportlab.pdfgen import canvas\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "\n",
    "# 🚀 Load GPT-2 model\n",
    "print(\"🚀 Loading GPT-2 model...\")\n",
    "gpt2_pil_generator = pipeline(\"text-generation\", model=\"./gpt2_pil_trained\")\n",
    "\n",
    "\n",
    "# 🎯 Improved GPT-2 generation with retries and stronger prompts\n",
    "def generate_gpt_section(prompt, section_name=\"\"):\n",
    "    \"\"\"Generate sections using GPT-2 with retries and better prompt control.\"\"\"\n",
    "    try:\n",
    "        result = gpt2_pil_generator(\n",
    "            prompt,\n",
    "            max_length=512,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "        generated_text = result[0][\"generated_text\"].replace(prompt, \"\").strip()\n",
    "\n",
    "        if not generated_text or len(generated_text) < 20:\n",
    "            print(f\"⚠️ {section_name} generation failed — retrying with stronger prompt...\")\n",
    "            refined_prompt = (\n",
    "                f\"{prompt}\\n\\nEnsure the output is legally accurate, clear, and enforceable.\"\n",
    "            )\n",
    "            refined_result = gpt2_pil_generator(\n",
    "                refined_prompt,\n",
    "                max_length=512,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.2,\n",
    "                num_return_sequences=1,\n",
    "                do_sample=True,\n",
    "                truncation=True,\n",
    "            )\n",
    "            generated_text = refined_result[0][\"generated_text\"].replace(refined_prompt, \"\").strip()\n",
    "\n",
    "        if not generated_text or len(generated_text) < 20:\n",
    "            return f\"⚠️ Unable to generate {section_name}. Please consult a legal expert.\"\n",
    "\n",
    "        return generated_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating {section_name}: {e}\")\n",
    "        return f\"⚠️ Error generating {section_name}. Please check the input.\"\n",
    "\n",
    "\n",
    "# 🛠️ Assemble the PIL Document\n",
    "def generate_pil(petitioner, respondent, subject, summary):\n",
    "    \"\"\"Create the full PIL document.\"\"\"\n",
    "\n",
    "    print(\"⚡ Generating Legal Grounds...\")\n",
    "    legal_grounds_prompt = (\n",
    "        f\"Draft strong, persuasive legal grounds for a Public Interest Litigation (PIL) on '{subject}', \"\n",
    "        f\"citing specific constitutional articles and legal principles.\"\n",
    "    )\n",
    "    legal_grounds = generate_gpt_section(legal_grounds_prompt, \"Legal Grounds\")\n",
    "\n",
    "    print(\"⚡ Generating Prayer...\")\n",
    "    prayer_prompt = f\"Draft a clear prayer for a PIL on '{subject}'.\"\n",
    "    prayer = generate_gpt_section(prayer_prompt, \"Prayer\")\n",
    "\n",
    "    print(\"⚡ Generating Court Procedure...\")\n",
    "    court_procedure_prompt = f\"Draft a step-by-step court procedure for a PIL on '{subject}'.\"\n",
    "    court_procedure = generate_gpt_section(court_procedure_prompt, \"Court Procedure\")\n",
    "\n",
    "    if \"⚠️\" in court_procedure:\n",
    "        court_procedure = (\n",
    "            \"1. File the PIL under Article 32 of the Constitution.\\n\"\n",
    "            \"2. Serve notice to the Respondent and the Attorney General.\\n\"\n",
    "            \"3. Attach supporting evidence and request an expedited hearing.\"\n",
    "        )\n",
    "\n",
    "    pil_text = f\"\"\"\n",
    "IN THE HON'BLE SUPREME COURT OF INDIA\n",
    "\n",
    "PUBLIC INTEREST LITIGATION (PIL)\n",
    "\n",
    "Petitioner: {petitioner}\n",
    "Respondent: {respondent}\n",
    "\n",
    "Subject: {subject}\n",
    "\n",
    "Respected Lordships,\n",
    "\n",
    "{summary}\n",
    "\n",
    "Legal Grounds:\n",
    "{legal_grounds}\n",
    "\n",
    "Prayer:\n",
    "{prayer}\n",
    "\n",
    "Court Procedure:\n",
    "{court_procedure}\n",
    "\n",
    "Date: {datetime.now().strftime(\"%A, %d %B %Y\")}\n",
    "\n",
    "Yours sincerely,\n",
    "{petitioner}\n",
    "    \"\"\"\n",
    "\n",
    "    return pil_text\n",
    "\n",
    "\n",
    "# 📄 Export PIL to TXT and PDF\n",
    "def export_pil(pil_text, filename=\"PIL_Document\"):\n",
    "    \"\"\"Export the PIL to text and PDF files.\"\"\"\n",
    "    with open(f\"{filename}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(pil_text)\n",
    "    print(f\"✅ PIL saved as {filename}.txt\")\n",
    "\n",
    "    pdf_file = f\"{filename}.pdf\"\n",
    "    c = canvas.Canvas(pdf_file, pagesize=A4)\n",
    "    c.setFont(\"Helvetica\", 12)\n",
    "    for i, line in enumerate(pil_text.split(\"\\n\")):\n",
    "        c.drawString(40, 800 - (i * 20), line)\n",
    "    c.save()\n",
    "    print(f\"✅ PIL saved as {pdf_file}\")\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)  # Enable CORS for frontend communication\n",
    "\n",
    "@app.route('/run_pil_generator', methods=['POST'])\n",
    "def run_pil_generator():\n",
    "    \"\"\"Run the PIL Generator interactively.\"\"\"\n",
    "    data = request.json\n",
    "    subject = data.get('subject')\n",
    "    petitioner = data.get('petitioner')\n",
    "    respondent = data.get('respondent')\n",
    "    summary = data.get('summary')\n",
    "\n",
    "    print(\"⚡ Generating the complete PIL document...\")\n",
    "    generated_pil = generate_pil(petitioner, respondent, subject, summary)\n",
    "\n",
    "    return jsonify({\"pil_text\": generated_pil})\n",
    "\n",
    "\n",
    "# 🔥 Run Flask Server\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, port=5001)  # Changed port to 5001 to avoid conflicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 150\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# 🔥 Run Flask Server\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 150\u001b[0m     app\u001b[38;5;241m.\u001b[39mrun(debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, port\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5001\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/flask/app.py:625\u001b[0m, in \u001b[0;36mFlask.run\u001b[0;34m(self, host, port, debug, load_dotenv, **options)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwerkzeug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run_simple\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 625\u001b[0m     run_simple(t\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;28mstr\u001b[39m, host), port, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;66;03m# reset the first request information if the development server\u001b[39;00m\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# reset normally.  This makes it possible to restart the server\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# without reloader and that stuff from an interactive shell.\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_got_first_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/werkzeug/serving.py:1106\u001b[0m, in \u001b[0;36mrun_simple\u001b[0;34m(hostname, port, application, use_reloader, use_debugger, use_evalex, extra_files, exclude_patterns, reloader_interval, reloader_type, threaded, processes, request_handler, static_files, passthrough_errors, ssl_context)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_reloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run_with_reloader\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1106\u001b[0m     run_with_reloader(\n\u001b[1;32m   1107\u001b[0m         srv\u001b[38;5;241m.\u001b[39mserve_forever,\n\u001b[1;32m   1108\u001b[0m         extra_files\u001b[38;5;241m=\u001b[39mextra_files,\n\u001b[1;32m   1109\u001b[0m         exclude_patterns\u001b[38;5;241m=\u001b[39mexclude_patterns,\n\u001b[1;32m   1110\u001b[0m         interval\u001b[38;5;241m=\u001b[39mreloader_interval,\n\u001b[1;32m   1111\u001b[0m         reloader_type\u001b[38;5;241m=\u001b[39mreloader_type,\n\u001b[1;32m   1112\u001b[0m     )\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1114\u001b[0m     srv\u001b[38;5;241m.\u001b[39mserver_close()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/werkzeug/_reloader.py:458\u001b[0m, in \u001b[0;36mrun_with_reloader\u001b[0;34m(main_func, extra_files, exclude_patterns, interval, reloader_type)\u001b[0m\n\u001b[1;32m    456\u001b[0m             reloader\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 458\u001b[0m         sys\u001b[38;5;241m.\u001b[39mexit(reloader\u001b[38;5;241m.\u001b[39mrestart_with_reloader())\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mSystemExit\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Loading GPT-2 model...\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5001\n",
      "Press CTRL+C to quit\n",
      " * Restarting with watchdog (fsevents)\n",
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1074, in launch_instance\n",
      "    app.initialize(argv)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 118, in inner\n",
      "    return method(app, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 654, in initialize\n",
      "    self.init_sockets()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 331, in init_sockets\n",
      "    self.shell_port = self._bind_socket(self.shell_socket, self.shell_port)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 253, in _bind_socket\n",
      "    return self._try_bind_socket(s, port)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 229, in _try_bind_socket\n",
      "    s.bind(\"tcp://%s:%i\" % (self.ip, port))\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/zmq/sugar/socket.py\", line 302, in bind\n",
      "    super().bind(addr)\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 564, in zmq.backend.cython.socket.Socket.bind\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 28, in zmq.backend.cython.checkrc._check_rc\n",
      "zmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002')\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import textwrap\n",
    "from datetime import datetime\n",
    "from transformers import pipeline\n",
    "from reportlab.lib.pagesizes import A4\n",
    "from reportlab.pdfgen import canvas\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "\n",
    "# 🚀 Load GPT-2 model\n",
    "print(\"🚀 Loading GPT-2 model...\")\n",
    "try:\n",
    "    gpt2_pil_generator = pipeline(\"text-generation\", model=\"./gpt2_pil_trained\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# 🎯 Improved GPT-2 generation with retries and better prompt handling\n",
    "def generate_gpt_section(prompt, section_name=\"\"):\n",
    "    \"\"\"Generate sections using GPT-2 with retries and better prompt control.\"\"\"\n",
    "    try:\n",
    "        result = gpt2_pil_generator(\n",
    "            prompt,\n",
    "            max_length=512,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "        generated_text = result[0][\"generated_text\"][len(prompt):].strip()\n",
    "\n",
    "        if len(generated_text) < 20:\n",
    "            print(f\"⚠️ {section_name} generation failed — retrying with stronger prompt...\")\n",
    "            refined_prompt = f\"{prompt}\\n\\nEnsure the output is legally accurate and enforceable.\"\n",
    "            refined_result = gpt2_pil_generator(\n",
    "                refined_prompt,\n",
    "                max_length=512,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.2,\n",
    "                num_return_sequences=1,\n",
    "                do_sample=True,\n",
    "                truncation=True,\n",
    "            )\n",
    "            generated_text = refined_result[0][\"generated_text\"][len(refined_prompt):].strip()\n",
    "\n",
    "        return generated_text if len(generated_text) >= 20 else f\"⚠️ Unable to generate {section_name}.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating {section_name}: {e}\")\n",
    "        return f\"⚠️ Error generating {section_name}.\"\n",
    "\n",
    "# 🛠️ Assemble the PIL Document\n",
    "def generate_pil(petitioner, respondent, subject, summary):\n",
    "    \"\"\"Create the full PIL document.\"\"\"\n",
    "\n",
    "    print(\"⚡ Generating Legal Grounds...\")\n",
    "    legal_grounds_prompt = (\n",
    "        f\"Draft strong, persuasive legal grounds for a Public Interest Litigation (PIL) on '{subject}', \"\n",
    "        f\"citing specific constitutional articles and legal principles.\"\n",
    "    )\n",
    "    legal_grounds = generate_gpt_section(legal_grounds_prompt, \"Legal Grounds\")\n",
    "\n",
    "    print(\"⚡ Generating Prayer...\")\n",
    "    prayer_prompt = f\"Draft a clear prayer for a PIL on '{subject}'.\"\n",
    "    prayer = generate_gpt_section(prayer_prompt, \"Prayer\")\n",
    "\n",
    "    print(\"⚡ Generating Court Procedure...\")\n",
    "    court_procedure_prompt = f\"Draft a step-by-step court procedure for a PIL on '{subject}'.\"\n",
    "    court_procedure = generate_gpt_section(court_procedure_prompt, \"Court Procedure\")\n",
    "\n",
    "    if \"⚠️\" in court_procedure:\n",
    "        court_procedure = (\n",
    "            \"1. File the PIL under Article 32 of the Constitution.\\n\"\n",
    "            \"2. Serve notice to the Respondent and the Attorney General.\\n\"\n",
    "            \"3. Attach supporting evidence and request an expedited hearing.\"\n",
    "        )\n",
    "\n",
    "    pil_text = f\"\"\"\n",
    "IN THE HON'BLE SUPREME COURT OF INDIA\n",
    "\n",
    "PUBLIC INTEREST LITIGATION (PIL)\n",
    "\n",
    "Petitioner: {petitioner}\n",
    "Respondent: {respondent}\n",
    "\n",
    "Subject: {subject}\n",
    "\n",
    "Respected Lordships,\n",
    "\n",
    "{summary}\n",
    "\n",
    "Legal Grounds:\n",
    "{legal_grounds}\n",
    "\n",
    "Prayer:\n",
    "{prayer}\n",
    "\n",
    "Court Procedure:\n",
    "{court_procedure}\n",
    "\n",
    "Date: {datetime.now().strftime(\"%A, %d %B %Y\")}\n",
    "\n",
    "Yours sincerely,\n",
    "{petitioner}\n",
    "    \"\"\"\n",
    "\n",
    "    return pil_text\n",
    "\n",
    "# 📄 Export PIL to TXT and PDF with text wrapping\n",
    "def export_pil(pil_text, filename=\"PIL_Document\"):\n",
    "    \"\"\"Export the PIL to text and PDF files.\"\"\"\n",
    "    with open(f\"{filename}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(pil_text)\n",
    "    print(f\"✅ PIL saved as {filename}.txt\")\n",
    "\n",
    "    pdf_file = f\"{filename}.pdf\"\n",
    "    c = canvas.Canvas(pdf_file, pagesize=A4)\n",
    "    c.setFont(\"Helvetica\", 12)\n",
    "\n",
    "    y_position = 800  # Initial Y position\n",
    "    for line in pil_text.split(\"\\n\"):\n",
    "        wrapped_lines = textwrap.wrap(line, width=90)\n",
    "        for wrapped_line in wrapped_lines:\n",
    "            c.drawString(40, y_position, wrapped_line)\n",
    "            y_position -= 20  # Move to next line\n",
    "            if y_position < 40:  # Avoid overflow\n",
    "                c.showPage()\n",
    "                c.setFont(\"Helvetica\", 12)\n",
    "                y_position = 800\n",
    "\n",
    "    c.save()\n",
    "    print(f\"✅ PIL saved as {pdf_file}\")\n",
    "\n",
    "# 🖥️ Flask API Setup\n",
    "app = Flask(__name__)\n",
    "CORS(app)  # Enable CORS for frontend communication\n",
    "\n",
    "@app.route('/run_pil_generator', methods=['POST'])\n",
    "def run_pil_generator():\n",
    "    \"\"\"Run the PIL Generator interactively.\"\"\"\n",
    "    data = request.json\n",
    "    subject = data.get('subject')\n",
    "    petitioner = data.get('petitioner')\n",
    "    respondent = data.get('respondent')\n",
    "    summary = data.get('summary')\n",
    "\n",
    "    if not all([subject, petitioner, respondent, summary]):\n",
    "        return jsonify({\"error\": \"All fields are required\"}), 400\n",
    "\n",
    "    print(\"⚡ Generating the complete PIL document...\")\n",
    "    generated_pil = generate_pil(petitioner, respondent, subject, summary)\n",
    "\n",
    "    return jsonify({\"pil_text\": generated_pil})\n",
    "\n",
    "# 🔥 Run Flask Server\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, port=5001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
